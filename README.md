# Sentiment-Analysis-Using-Audio-Dataset
This project focuses on analyzing emotions and sentiments from audio data. 
It leverages machine learning techniques to process speech signals, convert them into feature representations (e.g., spectrograms or Mel-frequency cepstral coefficients - MFCCs), and then classifies the sentiment (positive, negative, neutral, etc.) expressed in the audio. 
The model is trained on labeled audio datasets, using algorithms such as deep learning (e.g., CNNs) to predict sentiment based on acoustic features. 



